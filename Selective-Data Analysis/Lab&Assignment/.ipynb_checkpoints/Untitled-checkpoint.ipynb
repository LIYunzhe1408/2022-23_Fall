{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "605ce8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import copy\n",
    "\n",
    "# from TreeVisualization import createPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c995543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据  150朵鸢尾花,4种特征，3种类别(Iris-setosa, Iris-versicolor, Iris-virginica)\n",
    "def load_preprocess_data():\n",
    "    iris_data = []\n",
    "    cnt = 0\n",
    "    # 得到全部数据\n",
    "    with open(\"iris.csv\") as f:\n",
    "\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if cnt == 0:\n",
    "                cnt += 1\n",
    "                continue\n",
    "            tmp = line.strip().split(',')\n",
    "            for i, e in enumerate(tmp):\n",
    "                if i < 4:\n",
    "                    tmp[i] = float(e)\n",
    "            iris_data.append(tmp)\n",
    "    print(\"数据集：\\n\", iris_data)\n",
    "    # labels: sepal length（花萼长度） sepal width（花萼宽度） petal length（花瓣长度）, petal width（花瓣宽度）；单位:cm\n",
    "    iris_labels = [\"花萼长度\", \"花萼宽度\", \"花瓣长度\", \"花瓣宽度\"]\n",
    "\n",
    "    # 四种特征全为连续型数值，将其转换为离散型特征\n",
    "    # 先找出每列特征的最大最小值\n",
    "    feat_min = []\n",
    "    feat_max = []\n",
    "    for i in range(len(iris_data[0]) - 1):\n",
    "        feat_min.append(min([row[i] for row in iris_data]))\n",
    "        feat_max.append(max([row[i] for row in iris_data]))\n",
    "    print(\"\\n特征最大最小值：\")\n",
    "    print(\"feat_max\", feat_max)\n",
    "    print(\"feat_min\", feat_min)\n",
    "    # 每个特征设定一个阈值，小于该阈值特征置为0，否则为1，根据每个特征的最大最小值，将该特征区间等分成十份\n",
    "    # 尝试每一个节点作为阈值时，计算其划分的数据集信息熵的大小，选择熵最小的节点作为最终划分阈值\n",
    "    threshold = []\n",
    "    for i in range(len(feat_min)):\n",
    "        best_entropy = cal_shannon_entropy(iris_data)\n",
    "        thre_list = np.linspace(feat_min[i], feat_max[i], 11)\n",
    "        best_thre = thre_list[0]\n",
    "        for thre in thre_list:\n",
    "            t_iris = copy.deepcopy(iris_data)\n",
    "            discrete_dataset(t_iris, i, thre)\n",
    "            sub_dataset0 = split_dataset(t_iris, i, 0)\n",
    "            sub_dataset1 = split_dataset(t_iris, i, 1)\n",
    "            entropy = len(sub_dataset0) / float(len(t_iris)) * cal_shannon_entropy(sub_dataset0) + len(sub_dataset1) / float(len(t_iris)) * cal_shannon_entropy(sub_dataset1)\n",
    "            if entropy < best_entropy:\n",
    "                best_thre = thre\n",
    "                best_entropy = entropy\n",
    "        threshold.append(best_thre)\n",
    "    print(\"\\n离散化特征阈值threshold确定:\\n\", threshold)\n",
    "    #根据得到的阈值将数据集离散化\n",
    "    for i, thre in enumerate(threshold):\n",
    "        discrete_dataset(iris_data, i, thre)\n",
    "    return iris_data, iris_labels, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a98a1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据阈值将数据集特征离散化\n",
    "def discrete_dataset(dataset, axis, threshold):\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][axis] < threshold:\n",
    "            dataset[i][axis] = 0\n",
    "        else:\n",
    "            dataset[i][axis] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f82f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集（80%），测试集（20%)\n",
    "def get_train_test_dadaset(iris_data, split = 0.8):\n",
    "    num = len(iris_data)\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for i in range(num):\n",
    "        if i % 5 == 0:\n",
    "            test_data.append(iris_data[i])\n",
    "        else:\n",
    "            train_data.append(iris_data[i])\n",
    "    return train_data, test_data\n",
    "    # random.shuffle(iris_data) #随机打乱数组顺序\n",
    "    # return iris_data[:int(num * split)], iris_data[int(num * split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c68d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回第i个特征值等于value的子数据集，并把该特征列去掉\n",
    "def split_dataset(dataset, i, value):\n",
    "    subdateset = []\n",
    "    for row in dataset:\n",
    "        if row[i] == value:\n",
    "            t = []\n",
    "            t.extend(row[0 : i])\n",
    "            t.extend((row[i + 1 :]))\n",
    "            subdateset.append(t)\n",
    "    return subdateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5912221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照id3/c4.5算法选择最优特征\n",
    "def choose_feature_to_split(dataset, c45 = False):\n",
    "    length = len(dataset)\n",
    "    num_feature = len(dataset[0]) - 1   #特征个数\n",
    "    base_entropy = cal_shannon_entropy(dataset)\n",
    "    best_info_gain = 0\n",
    "    best_info_gain_ratio = 0\n",
    "    best_feature_index = -1\n",
    "    for feature_index in range(num_feature):\n",
    "        feat_list = [row[feature_index] for row in dataset]\n",
    "        feat_dic = Counter(feat_list) #统计特征种类及出现次数\n",
    "        cur_entropy = 0\n",
    "        cur_feat_entropy = 0 # 数据集关于当前特征的信息熵, 用于c4.5算法计算\n",
    "        for feat in feat_dic:\n",
    "            sub_dataset = split_dataset(dataset, feature_index, feat)\n",
    "            sub_entropy = cal_shannon_entropy(sub_dataset)\n",
    "            cur_entropy += feat_dic[feat] / float(length) * sub_entropy\n",
    "            cur_feat_entropy -= feat_dic[feat] / float(length) * np.log2(feat_dic[feat] / float(length))\n",
    "\n",
    "        # 按照c4.5算法规则选择特征\n",
    "        if c45 == True:\n",
    "            gain_ration = (base_entropy - cur_entropy) / cur_feat_entropy\n",
    "            if gain_ration > best_info_gain_ratio:\n",
    "                best_info_gain_ratio = gain_ration\n",
    "                best_feature_index = feature_index\n",
    "        else:\n",
    "            # 按照id3算法，如果当前特征信息增益最大，那么选择它为最优子特征\n",
    "            if base_entropy - cur_entropy > best_info_gain:\n",
    "                best_info_gain = base_entropy - cur_entropy\n",
    "                best_feature_index = feature_index\n",
    "    return best_feature_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度优先构建决策树\n",
    "def create_decision_tree(dataset, labels, c45 = False):\n",
    "    class_list = [row[-1] for row in dataset]\n",
    "    # 如果数据集全部是同一个类别，直接返回该类别\n",
    "    if len(class_list) == class_list.count(class_list[0]):\n",
    "        return class_list[0]\n",
    "    # 如果没有特征可以继续划分或者特征区分度为0，返回出现次数最多的类别\n",
    "    if len(dataset[0]) == 1 or choose_feature_to_split(dataset, c45) == -1:\n",
    "        class_dic = Counter(class_list)\n",
    "        return class_dic.most_common()[0][0]\n",
    "\n",
    "    # 根据id3/c4.5算法选择信息增益最高的特征作为当前分类特征\n",
    "    best_feature_index = choose_feature_to_split(dataset, c45)    #将c45改为True即使用c4.5规则选择分类特征\n",
    "    best_feature_label = labels[best_feature_index]\n",
    "\n",
    "    labels.remove(best_feature_label)\n",
    "    # 以字典的形式构建决策树\n",
    "    decision_tree = {best_feature_label : {}}\n",
    "    feat_list = [row[best_feature_index] for row in dataset]\n",
    "    feat_dic = Counter(feat_list)\n",
    "    # 根据当前最优特征划分子数据集\n",
    "    for feat in feat_dic:\n",
    "        sub_dataset = split_dataset(dataset, best_feature_index, feat)\n",
    "        sub_labels = labels[:]\n",
    "        # 递归构建子数据集决策树\n",
    "        decision_tree[best_feature_label][feat] = create_decision_tree(sub_dataset, sub_labels)\n",
    "    return decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7460ca5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集：\n",
      " [[5.1, 3.5, 1.4, 0.2, 'Iris-setosa'], [4.9, 3.0, 1.4, 0.2, 'Iris-setosa'], [4.7, 3.2, 1.3, 0.2, 'Iris-setosa'], [4.6, 3.1, 1.5, 0.2, 'Iris-setosa'], [5.0, 3.6, 1.4, 0.2, 'Iris-setosa'], [5.4, 3.9, 1.7, 0.4, 'Iris-setosa'], [4.6, 3.4, 1.4, 0.3, 'Iris-setosa'], [5.0, 3.4, 1.5, 0.2, 'Iris-setosa'], [4.4, 2.9, 1.4, 0.2, 'Iris-setosa'], [4.9, 3.1, 1.5, 0.1, 'Iris-setosa'], [5.4, 3.7, 1.5, 0.2, 'Iris-setosa'], [4.8, 3.4, 1.6, 0.2, 'Iris-setosa'], [4.8, 3.0, 1.4, 0.1, 'Iris-setosa'], [4.3, 3.0, 1.1, 0.1, 'Iris-setosa'], [5.8, 4.0, 1.2, 0.2, 'Iris-setosa'], [5.7, 4.4, 1.5, 0.4, 'Iris-setosa'], [5.4, 3.9, 1.3, 0.4, 'Iris-setosa'], [5.1, 3.5, 1.4, 0.3, 'Iris-setosa'], [5.7, 3.8, 1.7, 0.3, 'Iris-setosa'], [5.1, 3.8, 1.5, 0.3, 'Iris-setosa'], [5.4, 3.4, 1.7, 0.2, 'Iris-setosa'], [5.1, 3.7, 1.5, 0.4, 'Iris-setosa'], [4.6, 3.6, 1.0, 0.2, 'Iris-setosa'], [5.1, 3.3, 1.7, 0.5, 'Iris-setosa'], [4.8, 3.4, 1.9, 0.2, 'Iris-setosa'], [5.0, 3.0, 1.6, 0.2, 'Iris-setosa'], [5.0, 3.4, 1.6, 0.4, 'Iris-setosa'], [5.2, 3.5, 1.5, 0.2, 'Iris-setosa'], [5.2, 3.4, 1.4, 0.2, 'Iris-setosa'], [4.7, 3.2, 1.6, 0.2, 'Iris-setosa'], [4.8, 3.1, 1.6, 0.2, 'Iris-setosa'], [5.4, 3.4, 1.5, 0.4, 'Iris-setosa'], [5.2, 4.1, 1.5, 0.1, 'Iris-setosa'], [5.5, 4.2, 1.4, 0.2, 'Iris-setosa'], [4.9, 3.1, 1.5, 0.1, 'Iris-setosa'], [5.0, 3.2, 1.2, 0.2, 'Iris-setosa'], [5.5, 3.5, 1.3, 0.2, 'Iris-setosa'], [4.9, 3.1, 1.5, 0.1, 'Iris-setosa'], [4.4, 3.0, 1.3, 0.2, 'Iris-setosa'], [5.1, 3.4, 1.5, 0.2, 'Iris-setosa'], [5.0, 3.5, 1.3, 0.3, 'Iris-setosa'], [4.5, 2.3, 1.3, 0.3, 'Iris-setosa'], [4.4, 3.2, 1.3, 0.2, 'Iris-setosa'], [5.0, 3.5, 1.6, 0.6, 'Iris-setosa'], [5.1, 3.8, 1.9, 0.4, 'Iris-setosa'], [4.8, 3.0, 1.4, 0.3, 'Iris-setosa'], [5.1, 3.8, 1.6, 0.2, 'Iris-setosa'], [4.6, 3.2, 1.4, 0.2, 'Iris-setosa'], [5.3, 3.7, 1.5, 0.2, 'Iris-setosa'], [5.0, 3.3, 1.4, 0.2, 'Iris-setosa'], [7.0, 3.2, 4.7, 1.4, 'Iris-versicolor'], [6.4, 3.2, 4.5, 1.5, 'Iris-versicolor'], [6.9, 3.1, 4.9, 1.5, 'Iris-versicolor'], [5.5, 2.3, 4.0, 1.3, 'Iris-versicolor'], [6.5, 2.8, 4.6, 1.5, 'Iris-versicolor'], [5.7, 2.8, 4.5, 1.3, 'Iris-versicolor'], [6.3, 3.3, 4.7, 1.6, 'Iris-versicolor'], [4.9, 2.4, 3.3, 1.0, 'Iris-versicolor'], [6.6, 2.9, 4.6, 1.3, 'Iris-versicolor'], [5.2, 2.7, 3.9, 1.4, 'Iris-versicolor'], [5.0, 2.0, 3.5, 1.0, 'Iris-versicolor'], [5.9, 3.0, 4.2, 1.5, 'Iris-versicolor'], [6.0, 2.2, 4.0, 1.0, 'Iris-versicolor'], [6.1, 2.9, 4.7, 1.4, 'Iris-versicolor'], [5.6, 2.9, 3.6, 1.3, 'Iris-versicolor'], [6.7, 3.1, 4.4, 1.4, 'Iris-versicolor'], [5.6, 3.0, 4.5, 1.5, 'Iris-versicolor'], [5.8, 2.7, 4.1, 1.0, 'Iris-versicolor'], [6.2, 2.2, 4.5, 1.5, 'Iris-versicolor'], [5.6, 2.5, 3.9, 1.1, 'Iris-versicolor'], [5.9, 3.2, 4.8, 1.8, 'Iris-versicolor'], [6.1, 2.8, 4.0, 1.3, 'Iris-versicolor'], [6.3, 2.5, 4.9, 1.5, 'Iris-versicolor'], [6.1, 2.8, 4.7, 1.2, 'Iris-versicolor'], [6.4, 2.9, 4.3, 1.3, 'Iris-versicolor'], [6.6, 3.0, 4.4, 1.4, 'Iris-versicolor'], [6.8, 2.8, 4.8, 1.4, 'Iris-versicolor'], [6.7, 3.0, 5.0, 1.7, 'Iris-versicolor'], [6.0, 2.9, 4.5, 1.5, 'Iris-versicolor'], [5.7, 2.6, 3.5, 1.0, 'Iris-versicolor'], [5.5, 2.4, 3.8, 1.1, 'Iris-versicolor'], [5.5, 2.4, 3.7, 1.0, 'Iris-versicolor'], [5.8, 2.7, 3.9, 1.2, 'Iris-versicolor'], [6.0, 2.7, 5.1, 1.6, 'Iris-versicolor'], [5.4, 3.0, 4.5, 1.5, 'Iris-versicolor'], [6.0, 3.4, 4.5, 1.6, 'Iris-versicolor'], [6.7, 3.1, 4.7, 1.5, 'Iris-versicolor'], [6.3, 2.3, 4.4, 1.3, 'Iris-versicolor'], [5.6, 3.0, 4.1, 1.3, 'Iris-versicolor'], [5.5, 2.5, 4.0, 1.3, 'Iris-versicolor'], [5.5, 2.6, 4.4, 1.2, 'Iris-versicolor'], [6.1, 3.0, 4.6, 1.4, 'Iris-versicolor'], [5.8, 2.6, 4.0, 1.2, 'Iris-versicolor'], [5.0, 2.3, 3.3, 1.0, 'Iris-versicolor'], [5.6, 2.7, 4.2, 1.3, 'Iris-versicolor'], [5.7, 3.0, 4.2, 1.2, 'Iris-versicolor'], [5.7, 2.9, 4.2, 1.3, 'Iris-versicolor'], [6.2, 2.9, 4.3, 1.3, 'Iris-versicolor'], [5.1, 2.5, 3.0, 1.1, 'Iris-versicolor'], [5.7, 2.8, 4.1, 1.3, 'Iris-versicolor'], [6.3, 3.3, 6.0, 2.5, 'Iris-virginica'], [5.8, 2.7, 5.1, 1.9, 'Iris-virginica'], [7.1, 3.0, 5.9, 2.1, 'Iris-virginica'], [6.3, 2.9, 5.6, 1.8, 'Iris-virginica'], [6.5, 3.0, 5.8, 2.2, 'Iris-virginica'], [7.6, 3.0, 6.6, 2.1, 'Iris-virginica'], [4.9, 2.5, 4.5, 1.7, 'Iris-virginica'], [7.3, 2.9, 6.3, 1.8, 'Iris-virginica'], [6.7, 2.5, 5.8, 1.8, 'Iris-virginica'], [7.2, 3.6, 6.1, 2.5, 'Iris-virginica'], [6.5, 3.2, 5.1, 2.0, 'Iris-virginica'], [6.4, 2.7, 5.3, 1.9, 'Iris-virginica'], [6.8, 3.0, 5.5, 2.1, 'Iris-virginica'], [5.7, 2.5, 5.0, 2.0, 'Iris-virginica'], [5.8, 2.8, 5.1, 2.4, 'Iris-virginica'], [6.4, 3.2, 5.3, 2.3, 'Iris-virginica'], [6.5, 3.0, 5.5, 1.8, 'Iris-virginica'], [7.7, 3.8, 6.7, 2.2, 'Iris-virginica'], [7.7, 2.6, 6.9, 2.3, 'Iris-virginica'], [6.0, 2.2, 5.0, 1.5, 'Iris-virginica'], [6.9, 3.2, 5.7, 2.3, 'Iris-virginica'], [5.6, 2.8, 4.9, 2.0, 'Iris-virginica'], [7.7, 2.8, 6.7, 2.0, 'Iris-virginica'], [6.3, 2.7, 4.9, 1.8, 'Iris-virginica'], [6.7, 3.3, 5.7, 2.1, 'Iris-virginica'], [7.2, 3.2, 6.0, 1.8, 'Iris-virginica'], [6.2, 2.8, 4.8, 1.8, 'Iris-virginica'], [6.1, 3.0, 4.9, 1.8, 'Iris-virginica'], [6.4, 2.8, 5.6, 2.1, 'Iris-virginica'], [7.2, 3.0, 5.8, 1.6, 'Iris-virginica'], [7.4, 2.8, 6.1, 1.9, 'Iris-virginica'], [7.9, 3.8, 6.4, 2.0, 'Iris-virginica'], [6.4, 2.8, 5.6, 2.2, 'Iris-virginica'], [6.3, 2.8, 5.1, 1.5, 'Iris-virginica'], [6.1, 2.6, 5.6, 1.4, 'Iris-virginica'], [7.7, 3.0, 6.1, 2.3, 'Iris-virginica'], [6.3, 3.4, 5.6, 2.4, 'Iris-virginica'], [6.4, 3.1, 5.5, 1.8, 'Iris-virginica'], [6.0, 3.0, 4.8, 1.8, 'Iris-virginica'], [6.9, 3.1, 5.4, 2.1, 'Iris-virginica'], [6.7, 3.1, 5.6, 2.4, 'Iris-virginica'], [6.9, 3.1, 5.1, 2.3, 'Iris-virginica'], [5.8, 2.7, 5.1, 1.9, 'Iris-virginica'], [6.8, 3.2, 5.9, 2.3, 'Iris-virginica'], [6.7, 3.3, 5.7, 2.5, 'Iris-virginica'], [6.7, 3.0, 5.2, 2.3, 'Iris-virginica'], [6.3, 2.5, 5.0, 1.9, 'Iris-virginica'], [6.5, 3.0, 5.2, 2.0, 'Iris-virginica'], [6.2, 3.4, 5.4, 2.3, 'Iris-virginica'], [5.9, 3.0, 5.1, 1.8, 'Iris-virginica']]\n",
      "\n",
      "特征最大最小值：\n",
      "feat_max [7.9, 4.4, 6.9, 2.5]\n",
      "feat_min [4.3, 2.0, 1.0, 0.1]\n",
      "\n",
      "离散化特征阈值threshold确定:\n",
      " [5.74, 2.96, 2.18, 0.82]\n",
      "[0, 1, 0, 0, 'Iris-setosa']\n",
      "[0, 1, 0, 0, 'Iris-setosa']\n",
      "[0, 1, 0, 0, 'Iris-setosa']\n",
      "[0, 1, 0, 0, 'Iris-setosa']\n",
      "[0, 1, 0, 0, 'Iris-setosa']\n",
      "[0, 1, 0, 0, 'Iris-setosa']\n",
      "[0, 1, 0, 0, 'Iris-setosa']\n",
      "[0, 1, 0, 0, 'Iris-setosa']\n",
      "[0, 1, 0, 0, 'Iris-setosa']\n",
      "[0, 1, 0, 0, 'Iris-setosa']\n",
      "[1, 1, 1, 1, 'Iris-versicolor']\n",
      "[0, 0, 1, 1, 'Iris-versicolor']\n",
      "[0, 0, 1, 1, 'Iris-versicolor']\n",
      "[1, 1, 1, 1, 'Iris-versicolor']\n",
      "[1, 1, 1, 1, 'Iris-versicolor']\n",
      "[1, 1, 1, 1, 'Iris-versicolor']\n",
      "[0, 0, 1, 1, 'Iris-versicolor']\n",
      "[1, 1, 1, 1, 'Iris-versicolor']\n",
      "[0, 0, 1, 1, 'Iris-versicolor']\n",
      "[0, 1, 1, 1, 'Iris-versicolor']\n",
      "[1, 1, 1, 1, 'Iris-virginica']\n",
      "[1, 1, 1, 1, 'Iris-virginica']\n",
      "[1, 1, 1, 1, 'Iris-virginica']\n",
      "[1, 1, 1, 1, 'Iris-virginica']\n",
      "[1, 1, 1, 1, 'Iris-virginica']\n",
      "[1, 1, 1, 1, 'Iris-virginica']\n",
      "[1, 0, 1, 1, 'Iris-virginica']\n",
      "[1, 1, 1, 1, 'Iris-virginica']\n",
      "[1, 1, 1, 1, 'Iris-virginica']\n",
      "[1, 1, 1, 1, 'Iris-virginica']\n",
      "\n",
      "决策树字典形式表示：\n",
      " {'花瓣长度': {0: 'Iris-setosa', 1: {'花萼长度': {1: {'花萼宽度': {1: 'Iris-virginica', 0: 'Iris-versicolor'}}, 0: {'花萼宽度': {0: 'Iris-versicolor', 1: 'Iris-versicolor'}}}}}}\n",
      "[1, 1, 1, 1, 'Iris-versicolor']  wrong answer: Iris-virginica\n",
      "[1, 1, 1, 1, 'Iris-versicolor']  wrong answer: Iris-virginica\n",
      "[1, 1, 1, 1, 'Iris-versicolor']  wrong answer: Iris-virginica\n",
      "[1, 1, 1, 1, 'Iris-versicolor']  wrong answer: Iris-virginica\n",
      "[1, 1, 1, 1, 'Iris-versicolor']  wrong answer: Iris-virginica\n",
      "[1, 0, 1, 1, 'Iris-virginica']  wrong answer: Iris-versicolor\n",
      "\n",
      "预测准确率 : 80.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 根据决策树对数据进行预测\n",
    "def predict(tree, labels_index, input):\n",
    "    for key in tree:\n",
    "        if isinstance(tree[key][input[labels_index[key]]], str):\n",
    "            return tree[key][input[labels_index[key]]]\n",
    "        else:\n",
    "            return predict(tree[key][input[labels_index[key]]], labels_index, input)\n",
    "\n",
    "\n",
    "# 将决策树可视化\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset, labels, threshold = load_preprocess_data()\n",
    "    train_data, test_data = get_train_test_dadaset(dataset)\n",
    "    for t in test_data:\n",
    "        print(t)\n",
    "    # 构建决策树，用字典形式保存\n",
    "    decision_tree = create_decision_tree(train_data, labels, c45 = True)\n",
    "    print(\"\\n决策树字典形式表示：\\n\", decision_tree)\n",
    "\n",
    "    #测试集预测，计算准确率\n",
    "    labels_index = {\"花萼长度\" : 0, \"花萼宽度\" : 1, \"花瓣长度\" : 2, \"花瓣宽度\" : 3}\n",
    "    num = len(test_data)\n",
    "    correct_num = 0\n",
    "    for t in test_data:\n",
    "        if predict(decision_tree, labels_index, t) == t[-1]:\n",
    "            correct_num += 1\n",
    "        else:\n",
    "            print(t, \" wrong answer:\", predict(decision_tree, labels_index, t))\n",
    "\n",
    "    print(\"\\n预测准确率 : {:.2%}\".format(float(correct_num) / num))\n",
    "#     createPlot(decision_tree)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Jonas] *",
   "language": "python",
   "name": "conda-env-Jonas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
